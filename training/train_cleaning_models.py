import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
import joblib
import os
import datetime

# --- 1. è¨­å®šå¸¸é‡èˆ‡è·¯å¾‘ ---
FLOAT_FIELDS = [
    "gpu_usage_percent", "gpu_power_watt", "cpu_power_watt",
    "memory_used_mb", "disk_read_mb_s", "disk_write_mb_s", "system_power_watt"
]

# è‡ªå‹•ç”Ÿæˆå”¯ä¸€çš„ ID ä½œç‚ºç‰ˆæœ¬æ¨™è­˜ (æ—¥æœŸ_æ™‚é–“)
MODEL_ID = datetime.datetime.now().strftime("%Y%m%d_%H%M%S") 
print(f"Starting training for unique Model ID: {MODEL_ID}")

# è¨­å®šæ¨¡å‹è¼¸å‡ºè·¯å¾‘
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
OUTPUT_DIR = os.path.join(BASE_DIR, "models", "cleaning_models") 

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)


# --- 2. æ•¸æ“šè¼‰å…¥å‡½æ•¸ ---
def load_historical_data():
    """
    è¼‰å…¥æ­·å²æ•¸æ“šç”¨æ–¼è¨“ç·´ã€‚
    
    ã€é‡è¦ã€‘è«‹æ ¹æ“šæ‚¨çš„å¯¦éš›æƒ…æ³ä¿®æ”¹æ­¤å‡½æ•¸ï¼š
    1. å¾ CSV æª”æ¡ˆè®€å–
    2. å¾è³‡æ–™åº«æŸ¥è©¢
    3. å¾ API ç²å–
    
    ç¤ºä¾‹ï¼š
    df = pd.read_csv('historical_energy_data.csv')
    return df[FLOAT_FIELDS]
    """
    
    # === ä»¥ä¸‹æ˜¯æ¨¡æ“¬æ•¸æ“šï¼Œè«‹æ›¿æ›ç‚ºçœŸå¯¦æ•¸æ“šè¼‰å…¥é‚è¼¯ ===
    np.random.seed(42)
    n_samples = 5000  # å¢åŠ è¨“ç·´æ¨£æœ¬æ•¸
    
    data = {
        # æ¨¡æ“¬çœŸå¯¦çš„åŠŸè€—åˆ†å¸ƒ
        "cpu_power_watt": np.random.normal(25, 8, n_samples),      # å¹³å‡25Wï¼Œæ¨™æº–å·®8W
        "gpu_power_watt": np.random.normal(15, 10, n_samples),     # å¹³å‡15Wï¼Œæ¨™æº–å·®10W
        "system_power_watt": np.random.normal(150, 30, n_samples), # å¹³å‡150Wï¼Œæ¨™æº–å·®30W
        "gpu_usage_percent": np.random.beta(2, 5, n_samples) * 100, # åä½ä½¿ç”¨ç‡åˆ†å¸ƒ
        "memory_used_mb": np.random.normal(8000, 2000, n_samples),
        "disk_read_mb_s": np.random.exponential(5, n_samples),     # å¶çˆ¾é«˜å³°çš„åˆ†å¸ƒ
        "disk_write_mb_s": np.random.exponential(3, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # ç¢ºä¿æ•¸å€¼åœ¨åˆç†ç¯„åœå…§
    df['cpu_power_watt'] = df['cpu_power_watt'].clip(5, 125)
    df['gpu_power_watt'] = df['gpu_power_watt'].clip(0, 350)
    df['system_power_watt'] = df['system_power_watt'].clip(50, 500)
    df['gpu_usage_percent'] = df['gpu_usage_percent'].clip(0, 100)
    df['memory_used_mb'] = df['memory_used_mb'].clip(1000, 32000)
    df['disk_read_mb_s'] = df['disk_read_mb_s'].clip(0, 500)
    df['disk_write_mb_s'] = df['disk_write_mb_s'].clip(0, 500)
    
    # æ¨¡æ“¬ä¸€äº›ç¼ºå¤±å€¼ (ç´„5%)
    for field in FLOAT_FIELDS:
        missing_indices = np.random.choice(df.index, size=int(len(df) * 0.05), replace=False)
        df.loc[missing_indices, field] = np.nan
    
    # æ³¨å…¥å°‘é‡çœŸå¯¦ç•°å¸¸å€¼ (ç´„2%)
    anomaly_indices = np.random.choice(df.index, size=int(len(df) * 0.02), replace=False)
    df.loc[anomaly_indices, 'cpu_power_watt'] = np.random.uniform(150, 200, len(anomaly_indices))
    df.loc[anomaly_indices, 'system_power_watt'] = np.random.uniform(600, 800, len(anomaly_indices))
    
    print(f"Loaded {len(df)} training samples")
    print(f"Missing values: {df.isnull().sum().sum()} ({df.isnull().sum().sum() / df.size * 100:.2f}%)")
    
    return df


# --- 3. è¨“ç·´æµç¨‹ ---
def train_and_save_models(contamination_rate=0.03):
    """
    è¨“ç·´ä¸¦å„²å­˜ AI æ¸…æ´—æ¨¡å‹
    
    åƒæ•¸:
        contamination_rate: ç•°å¸¸å€¼æ¯”ä¾‹ (é è¨­ 0.03 = 3%)
                           - 0.01-0.02: éå¸¸åš´æ ¼ï¼Œé©åˆç©©å®šç’°å¢ƒ
                           - 0.03-0.05: å¹³è¡¡è¨­å®šï¼Œé©åˆä¸€èˆ¬ä½¿ç”¨
                           - 0.05-0.10: å¯¬é¬†è¨­å®šï¼Œé©åˆå¤šè®Šç’°å¢ƒ
    """
    print(f"\n{'='*60}")
    print(f"Training Configuration:")
    print(f"  - Contamination Rate: {contamination_rate*100:.1f}%")
    print(f"  - Model ID: {MODEL_ID}")
    print(f"{'='*60}\n")
    
    # è¼‰å…¥è¨“ç·´æ•¸æ“š
    historical_data = load_historical_data()
    data_for_training = historical_data[FLOAT_FIELDS].copy()
    
    # ç§»é™¤å®Œå…¨ç„¡æ•ˆçš„æ¨£æœ¬
    data_clean_for_fit = data_for_training.dropna()
    
    if data_clean_for_fit.empty:
        print("âŒ Error: Training data is too sparse or empty after dropping NaNs.")
        return
    
    print(f"Training samples after cleaning: {len(data_clean_for_fit)}")
    
    # --- è¨“ç·´ Scaler (ç”¨æ–¼æ¨™æº–åŒ–) ---
    print("\n[1/3] Training StandardScaler...")
    scaler = StandardScaler()
    scaler.fit(data_clean_for_fit) 
    scaler_path = os.path.join(OUTPUT_DIR, f'scaler_{MODEL_ID}.joblib')
    joblib.dump(scaler, scaler_path)
    print(f"  âœ“ Saved: {scaler_path}")
    
    # --- è¨“ç·´ M_A: ç•°å¸¸å€¼åµæ¸¬ (IsolationForest) ---
    print("\n[2/3] Training Anomaly Detection Model (IsolationForest)...")
    anomaly_model = IsolationForest(
        contamination=contamination_rate,  # èª¿æ•´æ•æ„Ÿåº¦
        random_state=42,
        n_estimators=150,      # å¢åŠ æ¨¹çš„æ•¸é‡æé«˜ç©©å®šæ€§
        max_samples='auto',    # è‡ªå‹•é¸æ“‡æ¨£æœ¬æ•¸
        max_features=1.0,      # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾µ
        bootstrap=False,       # ä¸ä½¿ç”¨è‡ªåŠ©æ³•æ¡æ¨£
        n_jobs=-1             # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
    )
    anomaly_model.fit(data_clean_for_fit)
    
    # è©•ä¼°æ¨¡å‹æ•ˆæœ
    predictions = anomaly_model.predict(data_clean_for_fit)
    anomaly_count = np.sum(predictions == -1)
    anomaly_ratio = anomaly_count / len(predictions)
    
    print(f"  âœ“ Training complete")
    print(f"  - Detected anomalies: {anomaly_count}/{len(predictions)} ({anomaly_ratio*100:.2f}%)")
    print(f"  - Expected rate: {contamination_rate*100:.1f}%")
    
    if abs(anomaly_ratio - contamination_rate) > 0.02:
        print(f"  âš ï¸  Warning: Actual anomaly rate differs significantly from expected")
    
    anomaly_path = os.path.join(OUTPUT_DIR, f'anomaly_model_{MODEL_ID}.joblib')
    joblib.dump(anomaly_model, anomaly_path)
    print(f"  âœ“ Saved: {anomaly_path}")

    # --- è¨“ç·´ M_I: æ™ºæ…§å¡«è£œ (KNNImputer) ---
    print("\n[3/3] Training Imputation Model (KNNImputer)...")
    imputer_model = KNNImputer(
        n_neighbors=5,
        weights='distance',  # ä½¿ç”¨è·é›¢åŠ æ¬Š
        metric='nan_euclidean'
    )
    
    # å…ˆæ¨™æº–åŒ–å†è¨“ç·´å¡«è£œå™¨
    data_scaled = scaler.transform(data_for_training.fillna(data_for_training.mean())) 
    imputer_model.fit(data_scaled) 
    
    imputer_path = os.path.join(OUTPUT_DIR, f'imputer_model_{MODEL_ID}.joblib')
    joblib.dump(imputer_model, imputer_path)
    print(f"  âœ“ Saved: {imputer_path}")
    
    # --- è¨“ç·´å®Œæˆç¸½çµ ---
    print(f"\n{'='*60}")
    print(f"âœ… Training Complete!")
    print(f"{'='*60}")
    print(f"\nğŸ“‹ Model Summary:")
    print(f"  Model ID: {MODEL_ID}")
    print(f"  Contamination Rate: {contamination_rate*100:.1f}%")
    print(f"  Training Samples: {len(data_clean_for_fit)}")
    print(f"  Detected Anomalies: {anomaly_count} ({anomaly_ratio*100:.2f}%)")
    print(f"\nğŸ“ Saved Models:")
    print(f"  - {scaler_path}")
    print(f"  - {anomaly_path}")
    print(f"  - {imputer_path}")
    print(f"\nğŸ”§ Next Step:")
    print(f"  Update your agent's ACTIVE_MODEL_ID to: {MODEL_ID}")
    print(f"\n  In integrated_agent.py, change:")
    print(f"    ACTIVE_MODEL_ID = \"{MODEL_ID}\"")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    # === èª¿æ•´æ­¤åƒæ•¸ä¾†æ§åˆ¶æ•æ„Ÿåº¦ ===
    # å»ºè­°å€¼ï¼š
    #   - 0.02: éå¸¸åš´æ ¼ï¼Œåªæ¨™è¨˜æ¥µç«¯ç•°å¸¸
    #   - 0.03: å¹³è¡¡è¨­å®šï¼ˆæ¨è–¦ï¼‰
    #   - 0.05: å¯¬é¬†è¨­å®š
    #   - 0.08: éå¸¸å¯¬é¬†
    
    CONTAMINATION_RATE = 0.03  # å¾ 0.0001 æ”¹ç‚º 0.03
    
    train_and_save_models(contamination_rate=CONTAMINATION_RATE)