import os
import random
import numpy as np
import pandas as pd
import joblib
from pathlib import Path
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from urllib.parse import urlparse
from datetime import datetime, timezone

# ========= éš¨æ©Ÿç¨®å­ï¼ˆå¯é‡ç¾ï¼‰ =========
np.random.seed(42)
random.seed(42)
tf.random.set_seed(42)

# ========= å…ˆè¼‰å…¥ .envï¼Œå†å–ç’°å¢ƒè®Šæ•¸ =========
load_dotenv()

DB_URL      = os.getenv("DB_URL")
STEP_MIN    = int(os.getenv("STEP_MINUTES", "1"))
WINDOW      = int(os.getenv("WINDOW", "72"))
EPOCHS      = int(os.getenv("EPOCHS", "20"))
BATCH       = int(os.getenv("BATCH", "128"))

# å¯é¸è¨“ç·´æœŸé–“åƒæ•¸
TRAIN_DAYS      = os.getenv("TRAIN_DAYS")
TRAIN_START_UTC = os.getenv("TRAIN_START_UTC")
TRAIN_END_UTC   = os.getenv("TRAIN_END_UTC")

# é˜²å‘†ï¼šå°å‡º DB_URLï¼ˆé®æ“‹å¯†ç¢¼ï¼‰
u = urlparse(DB_URL) if DB_URL else None
if not DB_URL:
    raise RuntimeError("DB_URL æœªè¨­å®šï¼Œè«‹åœ¨ training/.env è£œä¸Šã€‚")
print("Using DB_URL:", f"{u.scheme}://{u.username}:******@{u.hostname}:{u.port}{u.path}")
print(f"STEP_MIN={STEP_MIN}, WINDOW={WINDOW}, EPOCHS={EPOCHS}, BATCH={BATCH}, "
      f"TRAIN_DAYS={TRAIN_DAYS}, TRAIN_START_UTC={TRAIN_START_UTC}, TRAIN_END_UTC={TRAIN_END_UTC}")

# ========= æ¬„ä½åç¨± =========
TIME_COL = "timestamp"
PWR_COL  = "system_power_watt"
N_FEATURES = 3 # <<< é—œéµä¿®æ­£ï¼šç‰¹å¾µæ•¸é‡è®Šç‚º 3 (power_w, hour, dayofweek)

# ========= SQLï¼ˆtimestamp_utc::timestamptzï¼‰ =========
where_extra, params = [], {}
if TRAIN_DAYS and TRAIN_DAYS.isdigit():
    where_extra.append("(timestamp_utc)::timestamptz >= now() - interval :train_days")
    params["train_days"] = f"{int(TRAIN_DAYS)} days"
if TRAIN_START_UTC:
    where_extra.append("(timestamp_utc)::timestamptz >= :start_utc")
    params["start_utc"] = TRAIN_START_UTC
if TRAIN_END_UTC:
    where_extra.append("(timestamp_utc)::timestamptz <= :end_utc")
    params["end_utc"] = TRAIN_END_UTC

where_clause = (" AND " + " AND ".join(where_extra) + "\n") if where_extra else ""

SQL = f"""
SELECT
  (timestamp_utc)::timestamptz AS {TIME_COL},
  {PWR_COL}
FROM energy_cleaned
WHERE timestamp_utc IS NOT NULL
  AND {PWR_COL} IS NOT NULL
{where_clause}ORDER BY (timestamp_utc)::timestamptz
""".strip()
print("DEBUG SQL:\n", SQL)

# ========= è®€è³‡æ–™ =========
engine = create_engine(DB_URL, pool_pre_ping=True)
df = pd.read_sql(text(SQL), engine, params=params, parse_dates=[TIME_COL])
if df.empty:
    raise RuntimeError("energy_cleaned æŸ¥ç„¡è³‡æ–™ï¼ˆç¬¦åˆæ¢ä»¶ç‚º 0ï¼‰ã€‚")

print(f"Loaded rows: {len(df)} | time span: {df[TIME_COL].min()} â†’ {df[TIME_COL].max()}")

# ========= æ™‚é–“å°é½Šã€è£œå€¼èˆ‡ç‰¹å¾µå·¥ç¨‹ (é—œéµä¿®æ­£) =========
df = df.set_index(TIME_COL).sort_index()
df = df.resample(f"{STEP_MIN}min").mean()
df[PWR_COL] = df[PWR_COL].ffill().bfill()
df = df.dropna(subset=[PWR_COL])

# >>>>>>>>>>> ä¿®æ­£ 1ï¼šæ–°å¢æ™‚é–“ç‰¹å¾µ (æå‡æº–ç¢ºæ€§) <<<<<<<<<<<<
# ç¢ºä¿ç´¢å¼•æ˜¯ datetime é¡å‹
df.index = pd.to_datetime(df.index)

# å‰µå»º Hour (0-23) å’Œ DayofWeek (0=Monday, 6=Sunday)
df['hour'] = df.index.hour.astype(float)
df['dayofweek'] = df.index.dayofweek.astype(float)

# é¸æ“‡æ‰€æœ‰ç‰¹å¾µï¼šåŠŸè€—ã€å°æ™‚ã€æ˜ŸæœŸå¹¾
features = df[[PWR_COL, 'hour', 'dayofweek']].astype(float).values
# >>>>>>>>>>> ä¿®æ­£ 1 çµæŸ <<<<<<<<<<<<


# ========= è½‰ numpy + æ­£è¦åŒ–èˆ‡å»ºåºåˆ— (é—œéµä¿®æ­£) =========
if len(features) < WINDOW + 2:
    # ä¿®æ­£ï¼šå¦‚æœæ•¸æ“šé‡ä¸è¶³ï¼Œè‡ªå‹•èª¿æ•´ WINDOW å¤§å°
    new_window = max(1, int(len(features) * 0.7) - 2)
    if new_window < WINDOW:
        print(f"âš ï¸ è³‡æ–™é‡ ({len(features)}) ä¸è¶³ï¼Œè‡ªå‹•é™ä½ WINDOW: {WINDOW} â†’ {new_window}")
        WINDOW = new_window
    if len(features) < WINDOW + 2:
        raise RuntimeError(f"è³‡æ–™é‡ä¸è¶³ï¼ˆ{len(features)} < WINDOW+2ï¼‰è«‹å¢åŠ è³‡æ–™æˆ–é™ä½ WINDOWã€‚")

print(f"Total processed rows: {len(features)}")

# å…ˆåˆ‡åˆ†æ•¸æ“šï¼Œå†é€²è¡Œ Scaling (é¿å…è³‡æ–™æ´©æ¼)
split_idx = int(len(features) * 0.8)
features_tr, features_val = features[:split_idx], features[split_idx:]
print(f"Split index: {split_idx} / {len(features)}")

# ä¿®æ­£ 2Aï¼šè¨“ç·´ Scaler æ™‚ä½¿ç”¨æ‰€æœ‰ N_FEATURES
scaler = MinMaxScaler()
features_tr_scaled  = scaler.fit_transform(features_tr)
features_val_scaled = scaler.transform(features_val)

def make_sequences(arr, window, n_features):
    X, y = [], []
    for i in range(len(arr) - window - 1):
        X.append(arr[i:i+window])
        # ä¿®æ­£ 2Bï¼šé æ¸¬ç›®æ¨™ y ä»ç„¶æ˜¯åŠŸè€— (ç¬¬ 0 åˆ—)
        y.append(arr[i+window, 0]) 
    # X çš„å½¢ç‹€æ˜¯ (-1, WINDOW, N_FEATURES)
    return np.array(X).reshape(-1, window, n_features), np.array(y).reshape(-1, 1)

# ä¿®æ­£ 2Cï¼šå‘¼å« make_sequences æ™‚å‚³é N_FEATURES=3
X_tr, y_tr   = make_sequences(features_tr_scaled, WINDOW, N_FEATURES)
X_val, y_val = make_sequences(features_val_scaled, WINDOW, N_FEATURES)

if len(X_tr) == 0 or len(X_val) == 0:
    raise RuntimeError("åˆ‡åˆ†å¾Œç„¡æ³•å½¢æˆåºåˆ—ï¼Œè«‹èª¿æ•´è³‡æ–™é‡æˆ– WINDOWã€‚")

print(f"Train/Val shapes: X_tr={X_tr.shape}, X_val={X_val.shape}")

# ========= å»ºæ¨¡èˆ‡è¨“ç·´ (é—œéµä¿®æ­£) =========
# ä¿®æ­£ 3ï¼šLSTM è¼¸å…¥å½¢ç‹€å¿…é ˆåŒ¹é… N_FEATURES=3
model = Sequential([
    LSTM(64, input_shape=(WINDOW, N_FEATURES), return_sequences=False), # return_sequences=False for single output
    Dropout(0.1),
    Dense(1)
])
model.compile(optimizer="adam", loss="mse")

# ========= è¨“ç·´ï¼ˆLR èª¿åº¦ + EarlyStoppingï¼‰ =========
es  = EarlyStopping(patience=8, restore_best_weights=True)
rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5, verbose=1)

history = model.fit(
    X_tr, y_tr,
    validation_data=(X_val, y_val),
    epochs=EPOCHS, batch_size=BATCH,
    callbacks=[es, rlr],
    verbose=1
)

best_epoch = int(np.argmin(history.history["val_loss"]) + 1)

# ========= è©•ä¼°ï¼ˆåæ¨™æº–åŒ–ï¼‰(é—œéµä¿®æ­£) =========
y_pred = model.predict(X_val, verbose=0)

# ä¿®æ­£ 4ï¼šåæ¨™æº–åŒ–æ™‚å¿…é ˆå‰µå»ºåŒ…å«æ‰€æœ‰ç‰¹å¾µçš„è‡¨æ™‚é™£åˆ—
y_temp = np.zeros((len(y_val), N_FEATURES))
y_pred_temp = np.zeros((len(y_pred), N_FEATURES))

# å°‡åŸå§‹ y å€¼å’Œé æ¸¬ y å€¼æ”¾å…¥ç¬¬ 0 åˆ— (power_w çš„ä½ç½®)
y_temp[:, 0] = y_val.flatten()
y_pred_temp[:, 0] = y_pred.flatten()

# åå‘è½‰æ›æ™‚åªå–ç¬¬ 0 åˆ— (power_w)
y_val_w = scaler.inverse_transform(y_temp)[:, 0]
y_pred_w = scaler.inverse_transform(y_pred_temp)[:, 0]

rmse = float(np.sqrt(mean_squared_error(y_val_w, y_pred_w)))
mape = float(mean_absolute_percentage_error(y_val_w, y_pred_w) * 100)
best_val_loss = float(np.min(history.history["val_loss"]))
try:
    current_lr = float(tf.keras.backend.get_value(model.optimizer.learning_rate))
except Exception:
    current_lr = 0.0

print(f"âœ… æœ€ä½³ Epoch ç·¨è™Ÿï¼š{best_epoch} / {EPOCHS}")
print(f"RMSE(W): {rmse:.2f} | MAPE(%): {mape:.2f} | best_val_loss: {best_val_loss:.6f} | lr={current_lr:.6g}")

# ========= è¼¸å‡ºæ¨¡å‹èˆ‡ Scaler =========
out_dir = Path("/models")
if not out_dir.exists():
    out_dir = Path(__file__).resolve().parents[1] / "models"
out_dir.mkdir(parents=True, exist_ok=True)

model_path  = out_dir / "lstm_carbon_model.keras"
scaler_path = out_dir / "scaler_power.pkl"
model.save(model_path)
joblib.dump(scaler, scaler_path)
print("âœ… Saved:", model_path, "and", scaler_path)

# ========= è¨“ç·´æ—¥èªŒå¯«å…¥ /models/train_log.txt =========
log_path = out_dir / "train_log.txt"
data_start = df.index.min()
data_end   = df.index.max()
n_raw      = len(df)
n_tr       = len(X_tr)
n_val      = len(X_val)

log_line = (
    f"{datetime.now(timezone.utc).isoformat()} "
    f"| best_epoch={best_epoch}/{EPOCHS} best_val_loss={best_val_loss:.6f} "
    f"| RMSE={rmse:.3f}W MAPE={mape:.2f}% lr={current_lr:.6g} "
    f"| window={WINDOW} step_min={STEP_MIN} batch={BATCH} "
    f"| data_span=[{data_start} â†’ {data_end}] n_raw={n_raw} n_tr_seq={n_tr} n_val_seq={n_val} "
    f"| model={model_path.name} scaler={scaler_path.name}\n"
)

try:
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(log_line)
    print(f"ğŸ“ Appended train log â†’ {log_path}")
except Exception as e:
    print(f"âš ï¸ Failed to write train log: {e}")